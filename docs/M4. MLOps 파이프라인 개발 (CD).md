# M4. MLOps 파이프라인 개발 (CD)

## 4.1 모델 배포 (API 개발)

### API 개발 요소

1. 데이터 모델 개발
	- 추론 결과를 저장할 테이블과 관련된 테이블 모델 개발
	- API 입력값과 출력값의 데이터 모델 개발
2. 아티팩트 불러오기
	- 저장한 CatBoost 모델과 전처리 시 저장한 `RobustScaler` 불러오기
3. 모델 추론
	- 입력 받은 데이터로 추론 수행하여 결과 레이블과 그 확률값 반환
4. 로그 데이터 적재
	- 입력 받은 데이터, 결과 레이블, 확률값, 수행 시간 등을 생성해놓은 테이블에 적재
5. 컨테이너 개발
	- API 서비스를 띄워놓기 위한 Docker 컨테이너 개발
	- DAG 개발은 진행하지 않음

### BentoML (Remind)

![](https://i.imgur.com/VvfI2Py.png)

- **ML 모델 서빙만을 위한** 라이브러리
	- 대부분의 **메이저 ML 모델 라이브러리 지원**
	- 대부분의 퍼블릭 클라우드에서 사용 가능
	- 최근에는 LLMOps도 지원함
- 코드 기반으로 이후 Airflow 등 오케스트레이션 도구를 이용하여 Task로 만들 수 있음
- 배치 추론과 실시간 추론 모두 지원함
- 웹 대시보드로 모델 관리나 API 관리 가능
- 코드 몇 줄과 커맨드 몇 줄로 손쉽게 서빙 API 구축 가능

### API 폴더 구조

![](https://i.imgur.com/jPYb9ma.png)

### SQLAlchemy 연동

```python
feature_store_url = os.getenv("FEATURE_STORE_URL")

engine = create_engine(feature_store_url, echo=False)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()


def get_db():
    """데이터베이스 세션을 제공하는 함수 (의존성 주입용)"""
    with SessionLocal() as session:
        yield session
```

#### `sessionmaker`

- SQLAlchemy에서 **데이터베이스 세션을 생성하는 팩토리**
	- **DB와의 연결을 관리하는 세션을 쉽게 만들고 사용할 수 있도록 도와주는 도구**

#### `sessionmaker`의 필요성

1. DB 연결을 관리
	- SQLAlchemy에서는 데이터베이스와 직접 연결하지 않고, **세션(session)** 을 통해 데이터를 읽고 쓰는 작업을 하기 때문
2. 일관된 세션 생성
	- `sessionmaker`를 사용하면 동일한 설정을 가진 세션을 반복해서 쉽게 만들 수 있음
3. 트랜잭션 관리
	- `session.commit()` 또는 `session.rollback()`을 사용하여 데이터 변경 사항을 반영하거나 되돌릴 수 있음

#### 의존성 주입

- API가 직접 세션을 생성하지 않고 **외부에서 세션을 받아 유연성을 높이고 결합도를 낮춤**

#### Context Manager를 이용한 세션 관리

- `with` 절과 같은 context manager를 사용하면 세션을 사용한 다음 자동으로 세션을 닫을 수 있음

### 테이블 모델

```python
class CreditPredictionApiLog(Base):
	__tablename__ = "credit_predictions_api_log"
	
	id = Column(Integer, primary_key=True, autoincrement=True)
	customer_id = Column(String(10), nullable=False)
	features = Column(JSON, nullable=False)
	prediction = Column(String(10), nullable=False)
	confidence = Column(Float, nullable=False)
	elapsed_ms = Column(Integer, nullable=False)
	created_at = Column(DateTime, server_default=func.now())
```

- ORM 기능의 활용
- 테이블 이름을 설정하고 아래에 각 컬럼의 이름에 대해 알맞는 데이터 타입, PK 여부, auto increment 여부, NOT NULL 여부 등을 지정
- 이후 해당 클래스를 이용하여 어렵지 않게 데이터 추가, 삭제 등 가능

### 서비스 코드

```python
import os
import time
import warnings

import bentoml
import joblib
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from sqlalchemy.orm import Session

from api.src.db import get_db
from api.src.models import CreditPredictionApiLog
from api.src.schemas import Features, Response
from utils.dates import DateValues

warnings.filterwarnings(action="ignore")

# .env 파일 로드
load_dotenv()

MODEL_NAME = "credit_score_classification"
BASE_DT = DateValues.get_current_date()

artifacts_path = os.getenv("ARTIFACTS_PATH")
encoder_path = os.path.join(
    artifacts_path, "preprocessing", MODEL_NAME, BASE_DT, "encoders"
)


@bentoml.service(
    resources={"cpu": "2"},
    traffic={"timeout": 10},
)
class CreditScoreClassifier:
    def __init__(self, db: Session = next(get_db())):
        self.db = db
        self.bento_model = bentoml.models.get("credit_score_classifier:latest")
        self.robust_scalers = joblib.load(
            os.path.join(encoder_path, "robust_scaler.joblib")
        )
        self.model = bentoml.catboost.load_model(self.bento_model)

    @bentoml.api
    def predict(self, data: Features) -> Response:
        start_time = time.time()
        df = pd.DataFrame([data.model_dump()])
        customer_id = df.pop("customer_id")

        for col, scaler in self.robust_scalers.items():
            df[col] = scaler.transform(df[[col]])

        prob = np.max(self.model.predict(df, prediction_type="Probability"))
        label = self.model.predict(df, prediction_type="Class").item()
        elapsed_ms = (time.time() - start_time) * 1000

        record = CreditPredictionApiLog(
            customer_id=customer_id.item(),
            features=data.model_dump(),
            prediction=label,
            confidence=prob,
            elapsed_ms=elapsed_ms,
        )
        self.db.add(record)
        self.db.commit()

        return Response(customer_id=customer_id, predict=label, confidence=prob)

    @bentoml.api(route="/metadata", output_spec=dict)
    def metadata(self):
        """현재 컨테이너에서 서빙 중인 모델의 메타데이터 반환"""
        return {
            "model_name": self.bento_model.tag.name,
            "model_version": self.bento_model.tag.version,
            "params": self.bento_model.info.metadata,
            "creation_time": self.bento_model.info.creation_time,
        }

```

### Docker 설정 파일 개발

#### Dockerfile

```Dockerfile
FROM python:3.11-slim

ARG USER_HOME=/home/codespace
ARG UTIL_PATH=utils
ARG API_PATH=api

RUN groupadd --gid 1000 codespace \
    && useradd --uid 1000 --gid codespace --shell /bin/bash --create-home codespace

COPY --chown=codespace:codespace ${UTIL_PATH}/ ${USER_HOME}/utils
COPY --chown=codespace:codespace ${API_PATH}/bentofile.yaml ${USER_HOME}/
COPY --chown=codespace:codespace ${API_PATH}/requirements.txt ${USER_HOME}/
COPY --chown=codespace:codespace ${API_PATH}/services.py ${USER_HOME}/
COPY --chown=codespace:codespace ${API_PATH}/src/ ${USER_HOME}/${API_PATH}/src

RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

USER codespace

RUN pip install --no-cache-dir \
    --trusted-host pypi.org --trusted-host files.pythonhosted.org \
    -r ${USER_HOME}/requirements.txt

ENV PATH="${USER_HOME}/.local/bin:${PATH}"

WORKDIR ${USER_HOME}/${API_PATH}
```

#### `docker-compose.yml`

```yml
services:
  bentoml_service:
    build:
      context: ../..
      dockerfile: api/docker/Dockerfile
    image: credit_score_classification-deploy:latest
    container_name: credit_score_classification_deploy
    volumes:
      - ${HOME}/airflow/artifacts:/home/codespace/artifacts
      - ${HOME}/bentoml:/home/codespace/bentoml
    environment:
      PYTHONPATH: /home/codespace
      ARTIFACTS_PATH: /home/codespace/artifacts
      FEATURE_STORE_URL: mysql+pymysql://root:root@mariadb:3306/mlops
    ports:
      - "3000:3000"
    command: >
      bentoml serve services:CreditScoreClassifier
    networks:
      mlops_network:
networks:
  mlops_network:
    name: mlops_network
    external: true
```

## 4.2 지속적 배포 구현

### 지속적 배포 DAG

![](https://i.imgur.com/vkQl8TI.png)

1. API 상태를 체크하여 **API가 작동하지 않을 때는 바로 신규 모델 배포**
2. API가 활성화되어 있는 상태인 경우 **현재 배포되어 있는 모델의 학습일**과 **가장 최근에 학습된 모델의 학습일**을 체크
3. 두 학습일을 다음 Task로 넘겨 비교
4. **배포된 모델보다 학습된 모델이 더 최근에 생성되었거나 배포된 모델이 없다면** 최신 학습 모델로 모델 배포
5. **배포된 모델이 최신이거나 학습된 모델이 없는 경우** 배포 스킵

### 지속적 배포 DAG 개발 

```python
from datetime import datetime
from typing import List

import bentoml
import pendulum
import requests
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.models import Variable

from utils.callbacks import failure_callback, success_callback


local_timezone = pendulum.timezone("Asia/Seoul")
airflow_dags_path = Variable.get("AIRFLOW_DAGS_PATH")


def get_branch_by_api_status() -> List[str] | str:
    try:
        response = requests.get("http://localhost:3000/healthz")
        if response.status_code == 200:
            return [
                "get_deployed_model_creation_time",
                "get_latest_trained_model_creation_time",
            ]
        else:
            return "deploy_new_model"
    except Exception as e:
        print(f"API 통신이 이루어지지 않았습니다.: {e}")
        return "deploy_new_model"


def get_deployed_model_creation_time() -> datetime | None:
    """이미 배포된 모델의 `creation_time`을 조회합니다."""
    try:
        response = requests.post("http://localhost:3000/metadata")
        if response.status_code == 200:
            return datetime.strptime(
                response.json().get("creation_time"), "%Y-%m-%dT%H:%M:%S.%fZ"
            )
        else:
            print(
                f"`creation_time`을 불러올 수 없습니다.: {response.status_code}"
            )
            return None
    except Exception as e:
        print(f"배포된 모델의 API를 받아오지 못했습니다.: {e}")
        return None


def get_latest_trained_model_creation_time() -> datetime | None:
    """로컬 저장소에 저장된 최신 학습 모델의 `creation_time` 조회합니다."""
    try:
        bento_model = bentoml.models.get("credit_score_classifier:latest")
        return bento_model.info.creation_time.replace(tzinfo=None)
    except Exception as e:
        print(f"Error getting latest trained model creation time: {e}")
        return None


def decide_model_update(ti):
    """
    현재 배포된 모델과 로컬 최신 학습 모델의 creation_time 비교.
    배포된 모델이 오래되었으면 새로운 모델을 배포하도록 결정.
    """
    api_status = ti.xcom_pull(task_ids="get_branch_by_api_status")

    if api_status == "deploy_new_model":
        return "deploy_new_model"

    deployed_creation_time = ti.xcom_pull(
        task_ids="get_deployed_model_creation_time"
    )
    trained_creation_time = ti.xcom_pull(
        task_ids="get_latest_trained_model_creation_time"
    )

    print("deployed_creation_time", deployed_creation_time)
    print("trained_creation_time", trained_creation_time)

    if deployed_creation_time is None:
        print("There is no deployed model!")
        return "deploy_new_model"

    if (
        trained_creation_time is not None
        and trained_creation_time > deployed_creation_time
    ):
        print("Deployed model is already out-of-date.")
        return "deploy_new_model"

    print("Skip deployment.")
    return "skip_deployment"


with DAG(
    dag_id="credit_score_classification_cd",
    default_args={
        "owner": "user",
        "depends_on_past": False,
        "email": ["otzslayer@gmail.com"],
        "on_failure_callback": failure_callback,
        "on_success_callback": success_callback,
    },
    description="A DAG for continuous deployment",
    schedule=None,
    start_date=datetime(2025, 1, 1, tzinfo=local_timezone),
    catchup=False,
    tags=["lgcns", "mlops"],
) as dag:
    # API 상태 체크 결과 가져오기
    get_api_status_task = BranchPythonOperator(
        task_id="get_branch_by_api_status",
        python_callable=get_branch_by_api_status,
        provide_context=True,
    )

    # 현재 컨테이너에서 실행 중인 모델의 creation_time 가져오기
    get_deployed_model_creation_time_task = PythonOperator(
        task_id="get_deployed_model_creation_time",
        python_callable=get_deployed_model_creation_time,
    )

    # 로컬에서 최신 학습된 모델의 creation_time 가져오기
    get_latest_trained_model_creation_time_task = PythonOperator(
        task_id="get_latest_trained_model_creation_time",
        python_callable=get_latest_trained_model_creation_time,
    )

    # 모델을 업데이트할지 결정
    decide_update_task = BranchPythonOperator(
        task_id="decide_update",
        python_callable=decide_model_update,
        provide_context=True,
    )

    # 새로운 모델을 배포
    deploy_new_model_task = BashOperator(
        task_id="deploy_new_model",
        bash_command=f"cd {airflow_dags_path}/api/docker &&"
        "docker compose up --build --detach",
    )

    # 배포를 건너뛸 경우 실행할 더미 태스크
    skip_deployment_task = PythonOperator(
        task_id="skip_deployment",
        python_callable=lambda: print("No new model to deploy"),
    )

    # DAG 실행 순서 정의
    # 1️⃣ API가 정상 동작하지 않으면 즉시 배포
    get_api_status_task >> deploy_new_model_task

    # 2️⃣ API가 정상 동작하면 모델 생성 시간 비교 후 업데이트 결정
    (
        get_api_status_task
        >> [
            get_deployed_model_creation_time_task,
            get_latest_trained_model_creation_time_task,
        ]
        >> decide_update_task
    )

    # 3️⃣ decide_update_task의 결과에 따라 모델 배포 여부 결정
    decide_update_task >> [deploy_new_model_task, skip_deployment_task]
```

### HTTP 응답 상태 코드 


| 코드 | 의미                  | 설명                                                                    | 예시                                       |
| ---- | --------------------- | ----------------------------------------------------------------------- | ------------------------------------------ |
| 200  | OK                    | 요청이 성공적으로 처리됨 (일반적인 성공 응답)                           | 데이터 조회 성공                           |
| 201  | Created               | 요청이 성공적으로 처리되었으며, 새로운 리소스가 생성됨 (POST 요청 결과) | 데이터 생성 성공                           |
| 400  | Bad Request           | 클라이언트의 요청이 잘못됨 (유효하지 않은 데이터, 형식 오류 등)         | 잘못된 요청 (폼 입력 오류)                 |
| 401  | Unauthorized          | 인증이 필요함                                                           | 인증 필요 (로그인 필요)                    |
| 403  | Forbidden             | 접근이 금지됨 (권한 부족)                                               | 권한 부족 (접근 거부)                      |
| 404  | Not Found             | 요청한 리소스를 찾을 수 없음                                            | 리소스 없음 (잘못된 URL)                   |
| 500  | Internal Server Error | 서버 내부 오류 발생                                                     | 서버 오류 발생                             |
| 502  | Bad Gateway           | 서버가 게이트웨이 역할을 하며 다른 서버로부터 잘못된 응답을 받음        | 백엔드 서버의 잘못된 응답                  |
| 503  | Service Unavailable   | 서버가 과부하 상태이거나 유지보수 중이라 사용할 수 없음                 | 과부하나 자원 부족으로 인한 서버 응답 불가 |

### `PythonOperator` vs `BranchPythonOperator`

#### `PythonOperator`

- 역할
	- 일반적인 Python 함수를 실행하는 Operator
	- DAG 내에서 특정 Python 함수를 실행하고 결과 반환
- 특징
	- 단순히 지정된 Python 함수를 실행
	- 실행 결과를 사용할 수도 있고 사용하지 않을 수도 있음
	- **DAG의 흐름을 바꾸지 않음**

#### `BranchPythonOperator`

- 역할
	- 특정 조건에 따라 DAG의 실행 흐름을 **분기(branching)**할 수 있는 Operator
	- Python 함수의 반환 값이 실행할 다음 Task를 결정함
- 특징
	- DAG의 실행 흐름을 동적으로 변경할 수 있음
	- **하나 이상의 Task ID를 반환**해야 함
	- 선택된 Task만 실행되며, 선택되지 않은 Task는 Skipped 상태가 됨
	- 인자로 `provide_context` 추가하여 다음 실행할 Task 정보를 보냄