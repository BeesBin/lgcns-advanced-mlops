# M3. MLOps 파이프라인 개발 (CT)

## 3.1 데이터 추출

### Airflow 추가 설정

- Admin → Connection에서 MySQL 커넥션 추가
	- Connection Id : `feature_store`
	- Connection Type : `MySQL`
	- Host : `0.0.0.0`
	- Schema : `mlops`
	- Login : `root`
	- Password : `root`
	- Port : `3306`

### 데이터 병합

- `credit_score` 테이블과 `credit_score_feature` 테이블을 병합해서 타겟값까지 포함하고 있는 테이블 생성
	- 최근 일주일 데이터만 포함하고 있는 테이블로 관리할 예정
		- 오늘 날짜의 데이터가 있다면 해당 데이터를 지우고 다시 추가
	- 아래와 같이 날짜를 Jinja template 으로 작성하는 쿼리 사용

```sql
-- pipelines/continuous_training/data_extract/features.sql
-- 1. 일주일 전 날짜 이전 데이터 삭제
DELETE FROM mlops.credit_score_features_target
WHERE base_dt <= DATE_FORMAT(
        DATE_ADD(
            STR_TO_DATE('{{ ds }}', '%Y-%m-%d'),
            INTERVAL -7 DAY
        ),
        '%Y-%m-%d'
    );

-- 2. 새로운 데이터 삽입
INSERT INTO mlops.credit_score_features_target (
        base_dt,
        id,
        customer_id,
        date,
        age,
        occupation,
        annual_income,
        monthly_inhand_salary,
        num_bank_accounts,
        num_credit_card,
        interest_rate,
        num_of_loan,
        type_of_loan,
        delay_from_due_date,
        num_of_delayed_payment,
        changed_credit_limit,
        num_credit_inquiries,
        credit_mix,
        outstanding_debt,
        credit_utilization_ratio,
        credit_history_age,
        payment_of_min_amount,
        total_emi_per_month,
        amount_invested_monthly,
        payment_behaviour,
        monthly_balance,
        credit_score
    )
SELECT STR_TO_DATE('{{ ds }}', '%Y-%m-%d') AS base_dt,
    b.id,
    b.customer_id,
    b.date,
    a.age,
    a.occupation,
    a.annual_income,
    a.monthly_inhand_salary,
    a.num_bank_accounts,
    a.num_credit_card,
    a.interest_rate,
    a.num_of_loan,
    a.type_of_loan,
    a.delay_from_due_date,
    a.num_of_delayed_payment,
    a.changed_credit_limit,
    a.num_credit_inquiries,
    a.credit_mix,
    a.outstanding_debt,
    a.credit_utilization_ratio,
    a.credit_history_age,
    a.payment_of_min_amount,
    a.total_emi_per_month,
    a.amount_invested_monthly,
    a.payment_behaviour,
    a.monthly_balance,
    b.credit_score
FROM mlops.credit_score_features a
    INNER JOIN (
        SELECT *
        FROM mlops.credit_score
        WHERE date BETWEEN DATE_ADD(
                STR_TO_DATE('{{ ds }}', '%Y-%m-%d'),
                INTERVAL -1 MONTH
            )
            AND STR_TO_DATE('{{ ds }}', '%Y-%m-%d')
    ) b ON a.id = b.id
    AND a.customer_id = b.customer_id;
```

### DAG 개발

- DAG 개발 순서는 다음과 같음
	- `EmptyOperator` 이용해서 빈 Task로 큰 틀을 구성
	- 각 Task를 추가 작성
	- 추가 작성 후 테스트
	- 문제 없으면 다음 Task 개발

```python
from datetime import datetime

import pendulum
from airflow import DAG
from airflow.operators.empty import EmptyOperator

from utils.callbacks import failure_callback, success_callback

local_timezone = pendulum.timezone("Asia/Seoul")

with DAG(
    dag_id="credit_score_classification_ct",
    default_args={
        "owner": "user",
        "depends_on_past": False,
        "email": ["otzslayer@gmail.com"],
        "on_failure_callback": failure_callback,
        "on_success_callback": success_callback,
    },
    description="A DAG for continuous training",
    schedule=None,
    start_date=datetime(2025, 1, 1, tzinfo=local_timezone),
    catchup=False,
    tags=["lgcns", "mlops"],
) as dag:
    data_extract = EmptyOperator(task_id="데이터_추출")

    data_preprocessing = EmptyOperator(task_id="데이터_전처리")

    training = EmptyOperator(task_id="모델_학습_및_평가")

    data_extract >> data_preprocessing >> training

```

- 이후 아래와 같이 데이터 추출 관련 코드 작성 

```python
import os
from datetime import datetime

import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator

from utils.callbacks import failure_callback, success_callback
from utils.common import read_sql_file

local_timezone = pendulum.timezone("Asia/Seoul")
conn_id = "feature_store"
airflow_dags_path = Variable.get("AIRFLOW_DAGS_PATH")
sql_file_path = os.path.join(
    airflow_dags_path,
    "pipelines",
    "continuous_training",
    "data_extract",
    "features.sql",
)

with DAG(
    dag_id="credit_score_classification_ct",
    default_args={
        "owner": "user",
        "depends_on_past": False,
        "email": ["otzslayer@gmail.com"],
        "on_failure_callback": failure_callback,
        "on_success_callback": success_callback,
    },
    description="A DAG for continuous training",
    schedule=None,
    start_date=datetime(2025, 1, 1, tzinfo=local_timezone),
    catchup=False,
    tags=["lgcns", "mlops"],
) as dag:

    data_extract = SQLExecuteQueryOperator(
        task_id="데이터_추출",
        conn_id=conn_id,
        sql=read_sql_file(sql_file_path),
        split_statements=True,
    )

    data_preprocessing = EmptyOperator(task_id="데이터_전처리")

    training = EmptyOperator(task_id="모델_학습_및_평가")

    data_extract >> data_preprocessing >> training

```

- `SQLExecuteQueryOperator` 에 사용되는 파라미터

| **파라미터**     | **타입**         | **설명**                                                                            |
| ---------------- | ---------------- | ----------------------------------------------------------------------------------- |
| `task_id`          | `str`              | DAG에서 사용될 Task의 ID                                                          |
| `conn_id`          | `str`              | Airflow에서 정의한 **연결 ID** (Airflow UI > Admin > Connections에서 확인 가능)     |
| `sql`              | `str` or `list`      | 실행할 SQL 쿼리 (문자열 또는 SQL 파일 경로 가능)                                    |
| `parameters`       | `dict` or `iterable` | SQL에 전달할 파라미터 (sqlalchemy.text 스타일 사용 가능)                            |
| `autocommit`       | `bool`             | 트랜잭션을 자동으로 커밋할지 여부 (기본값: False)                                   |
| `split_statements` | `bool`             | 여러 개의 SQL 문장을 하나의 리스트로 제공할 경우 **분할 실행 여부** (기본값: False) |
| `return_last`      | `bool`             | XCom에 저장할 때 마지막 쿼리의 결과만 저장할지 여부 (기본값: True)                  |
| `handler`          | `callable`         | 쿼리 결과를 가공할 핸들러 함수 (예: lambda cursor: cursor.fetchall())               |
| `database`         | `str`              | 특정 데이터베이스를 명시할 경우 사용 (MySQL, Postgres 등에서 지원)                  |

## 3.2 데이터 전처리

### 전처리 클래스 개발 로직

1. 데이터 불러오기
	- 만약 불러온 데이터가 비어있다면 오류 발생
2. 수치형 변수에 대해서 `RobustScaler` 적용
	- 추후 서빙 시 각 변수에 대한 `RobustScaler`를 불러와서 적용해야 하기 떄문에 객체 덤프 필요
		- 아티팩트 폴더에 저장
	- 학습/검증 데이터에 대해 적합시키고 변환 수행
3. 변환한 학습/검증 데이터를 아티팩트 폴더에 저장 
4. 파일 실행 부분에서 모델 이름과 실행 날짜를 인자로 받아서 아티팩트 폴더 아래에 관리

### 환경 변수 관리 방안

- 개발 환경에서는 프로젝트 루트 폴더에 `.env` 파일 생성해서 환경 변수 작성해 쉽게 관리 가능
	- `python-dotenv` 라이브러리의 `load_dotenv()` 함수 사용
	- 운영 환경에서는 보안에 취약하기 떄문에 다른 방법을 사용해야 함
		- 특히 `.env` 파일을 Git에서 추적하게 해놓으면 큰 문제가 생길 수 있음
- 다른 방법
	- Airflow를 사용하는 경우에는 Variables나 Secrets에서 관리할 수 있음
	- `sudo` 권한이 있는 경우 `/etc/environment` 에 추가하여 시스템 환경변수로 추가할 수 있음
	- 각 CSP에서 제공하는 기능 활용

### 도커 설정파일 개발

- `pipelines/continuous_training` 디렉토리에 `docker` 라는 폴더로 이동
	- 폴더 안에 `requirements.txt`, `Dockerfile`, `docker-compose.yml` 파일 수정
	- 이 파일들로 전처리, 학습 모듈을 모두 띄울 수 있음

### DAG 개발

```python
from airflow.operators.bash import BashOperator

data_preprocessing = BashOperator(
	task_id="데이터_전처리",
	bash_command=f"cd {airflow_dags_path}/pipelines/continuous_training/docker &&"
	"docker compose up --build && docker compose down",
	env={
		"PYTHON_FILE": "/home/codespace/data_preprocessing/preprocessor.py",
		"MODEL_NAME": "credit_score_classification",
		"BASE_DT": "{{ ds }}",
	},
	append_env=True,
	retries=1,
)
```

## 3.3 모델 학습/평가

### 모델 학습/평가 클래스 개발 로직

1. 데이터 불러오기
	- 불러온 후 학습 데이터와 검증 데이터에서 피처와 타겟값을 분리
	- CatBoost에서 사용 가능한 형태로 변환 `(catboost.Pool)`
2. 하이퍼파라미터 튜닝
	- CatBoost 모델을 학습하며, Grid Search로 최적의 하이퍼파라미터 탐색
3. 학습 결과를 MLflow와 아티팩트 폴더에 저장
	- 튜닝 중 매 시행에 대한 메타데이터를 MLflow에 저장하고 모델을 아티팩트 폴더에 저장
4. 최적 모델을 저장
	- 추후 배포를 위해 최적 모델을 BentoML로 저장
5. DAG 내 Task 업데이트
	- 데이터 전처리 때와 유사하게 이미지를 빌드하고 컨테이너를 띄우는 명령어로 구성된 Task 업데이트
	- 일부 구성 업데이트

### 도커 설정파일 개발

- `docker-compose.yml` 파일에 다음과 같이 볼륨을 추가 설정
	- `${HOME}/bentoml:/home/mlops/bentoml`
		- 컨테이너 내에서 `/home/mlops/bentoml`에 저장될 모델을 로컬에서 사용하도록 함
- `Dockerfile`에 학습 관련 폴더를 복사하는 명령 추가
	- `ARG TRAINING_PATH=pipelines/continuous_training/training`
	- `COPY --chown=mlops:mlops ${TRAINING_PATH}/trainer.py ${USER_HOME}/training/`

### DAG 개발

```python
training = BashOperator(
	task_id="모델_학습_및_평가",
	bash_command=f"cd {airflow_dags_path}/pipelines/continuous_training/docker &&"
	"docker compose up --build && docker compose down",
	env={
		"PYTHON_FILE": "/home/codespace/training/trainer.py",
		"MODEL_NAME": "credit_score_classification",
		"BASE_DT": "{{ ds }}",
	},
	append_env=True,
	retries=1,
)
```

- DAG 실행 후 Codespace에서 `~/bentoml/models` 에 올바르게 모델이 저장되었는지 확인
- 만약 오류가 발생하였다면
	- `~/airflow/artifacts` 와 `~/bentoml` 가 생성되어 있지 않아 Airflow가 `root` 권한으로 해당 폴더를 생성하면서 권한 문제가 발생했을 가능성 있음
	- 테스트 용도로 먼저 내부에서 코드를 돌려봤거나, 미리 폴더를 생성했다면 문제가 발생하지 않음
- 이미 에러가 발생해서 폴더를 새로 생성하기 꺼려진다면?
	- `sudo chown codespace:codespace -R ~/bentoml`
	- `sudo chown codespace:codespace -R ~/airflow/artifacts`